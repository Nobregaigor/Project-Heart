{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file compares the results from project_heart to data manually extracted from XPLT file. This data mimics the idea of speckles, and, for a fair comparison, it should reflect same speckles as those used to extract data. Due to complexity of longitudinal length, some simplification is made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_heart.lv import LV\n",
    "import numpy as np\n",
    "import pyvista as pv\n",
    "pv.set_jupyter_backend(\"pythreejs\")\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from project_heart.enums import *\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LV and extract metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_heart.examples import get_lv_typeA\n",
    "lv = get_lv_typeA(filepath=\"C:/Users/igorp/Downloads/0.28_70.00_40.00_LVMYO_HEX8_83648_wr.xplt\", statesfile=None,\n",
    "                  id_region_kwargs={\n",
    "                        \"border_thresh_base\":1.5, \n",
    "                        \"border_thresh_endo\":1.5, \n",
    "                        \"border_thresh_epi\":1.5,\n",
    "                  })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv.plot(\"surface\", \n",
    "        scalars=LV_MESH_DATA.SURFS_DETAILED,\n",
    "        categorical=True,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create speckles that match manual extracted nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = [\"subapex\", \"apex\", \"superapex\", \"submid\", \"mid\", \"supermid\", \"subbase\", \"base\", \"superbase\"]\n",
    "k = 0.89\n",
    "d = 1.5\n",
    "spks1 = lv.create_speckles(\n",
    "            collection=\"MANUAL_CALC_CIRC_6\",\n",
    "            group=\"endo\",\n",
    "            name=\"superbase\",\n",
    "            from_nodeset=LV_SURFS.ENDO,\n",
    "            d=d,\n",
    "            k=k,\n",
    "            normal_to=[0.0, 0.0, 1.0],\n",
    "            n_subsets=6,\n",
    "            subsets_criteria=\"angles\",\n",
    "            cluster_criteria=\"angles3\",\n",
    "            n_clusters=16,\n",
    "            t=0.0,\n",
    "        )\n",
    "spks2 = lv.create_speckles(\n",
    "            collection=\"MANUAL_CALC_CIRC_6\",\n",
    "            group=\"epi\",\n",
    "            name=\"superbase\",\n",
    "            from_nodeset=LV_SURFS.EPI,\n",
    "            d=d,\n",
    "            k=k,\n",
    "            normal_to=[0.0, 0.0, 1.0],\n",
    "            n_subsets=6,\n",
    "            subsets_criteria=\"angles\",\n",
    "            cluster_criteria=\"angles3\",\n",
    "            n_clusters=16,\n",
    "            t=0.0,\n",
    "        )\n",
    "\n",
    "spks3 = lv.create_speckles(\n",
    "            collection=\"MANUAL_CALC_CIRC_1\",\n",
    "            group=\"endo\",\n",
    "            name=\"superbase\",\n",
    "            from_nodeset=LV_SURFS.ENDO,\n",
    "            d=d,\n",
    "            k=k,\n",
    "            normal_to=[0.0, 0.0, 1.0],\n",
    "            n_subsets=0,\n",
    "            subsets_criteria=\"angles\",\n",
    "            cluster_criteria=\"angles3\",\n",
    "            n_clusters=50,\n",
    "            t=0.0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spks4 = lv.create_speckles(\n",
    "            collection=\"MANUAL_CALC_LONG_1\",\n",
    "            group=\"endo\",\n",
    "            name=\"0.0\",\n",
    "            from_nodeset=LV_SURFS.ENDO,\n",
    "            exclude_nodeset=LV_SURFS.BASE, # does not afect ideal case\n",
    "            d=1.3,\n",
    "            k=0.5,\n",
    "            normal_to=[1.0, 0.0, 0.0],\n",
    "            n_subsets=0,\n",
    "            subsets_criteria=\"z2\",\n",
    "            n_clusters=75,\n",
    "            cluster_criteria=\"angles3\",\n",
    "            clusters_vector_ref=np.asarray([0,0,1]),\n",
    "            t=0.0,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lv.plot_speckles(spks4, show_clusters=True, show_clusters_centers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lv.extract_geometrics({\n",
    "        \"volume\":{},\n",
    "        \"apex_base_over_timesteps\": {\n",
    "            \"apex_spks\": {\"spk_name\":\"apex\", \"spk_group\":\"endo\", \"spk_collection\":\"LA\"},\n",
    "            \"base_spks\": {\"spk_name\":\"base\", \"spk_group\":\"endo\", \"spk_collection\":\"LA\"},\n",
    "            },\n",
    "        \"longitudinal_distance\": {\n",
    "            \"apex_spks\": {\"spk_name\":\"apex\", \"spk_collection\":\"LA\"},\n",
    "            \"base_spks\": {\"spk_name\":\"base\", \"spk_collection\":\"LA\"},\n",
    "            \"approach\":\"centroid\", \n",
    "            \"use_axis_aligment\":False,\n",
    "            \"reduce_by\":{\"group\"}\n",
    "            },\n",
    "        \"radial_distance\": {\n",
    "            \"spks\": {\"spk_collection\":\"MANUAL_CALC_CIRC_6\"},\n",
    "            \"approach\":\"moving_vector\",\n",
    "            \"reduce_by\":{\"group\", \"name\", \"group_name\"},\n",
    "            },\n",
    "        \"radial_length\": {\n",
    "            \"spks\": {\"spk_collection\":\"MANUAL_CALC_CIRC_6\"},\n",
    "            \"approach\":\"moving_centers\",\n",
    "            \"reduce_by\":{\"group\", \"name\", \"group_name\"},\n",
    "            },\n",
    "        \"wall_thickness\": {\n",
    "            \"endo_spks\": {\"spk_group\":\"endo\", \"spk_collection\":\"MANUAL_CALC_CIRC_6\"},\n",
    "            \"epi_spks\": {\"spk_group\":\"epi\", \"spk_collection\":\"MANUAL_CALC_CIRC_6\"},\n",
    "            \"approach\":\"radial_distance\",\n",
    "            \"reduce_by\":{\"name\"},\n",
    "            },\n",
    "        \"longitudinal_length\": {\n",
    "            \"spks\": {\"spk_collection\":\"long-6\"},\n",
    "            \"approach\":\"k_ids\",\n",
    "            \"reduce_by\":{\"subset\"},\n",
    "            \"merge_subset\":{\n",
    "                    5: 0, \n",
    "                    4: 1, \n",
    "                    3: 2\n",
    "                },\n",
    "            },\n",
    "        \"global_longitudinal_length\": {\n",
    "            \"spks\": {\"spk_collection\":\"MANUAL_CALC_LONG_1\"},\n",
    "            \"approach\":\"k_ids\",\n",
    "            \"reduce_by\":{\"group\", \"group_name\"},\n",
    "            },\n",
    "        \"circumferential_length\": {\n",
    "            \"spks\": {\"spk_collection\":\"MANUAL_CALC_CIRC_6\"},\n",
    "            \"reduce_by\":{\"group\"},\n",
    "            },\n",
    "        \"global_circumferential_length\": {\n",
    "            \"spks\": {\"spk_collection\":\"MANUAL_CALC_CIRC_1\"},\n",
    "            \"reduce_by\":{\"group\", \"group_name\"},\n",
    "            },\n",
    "        \"angle_rotation\": {\n",
    "            \"spks\": {\"spk_collection\":\"MANUAL_CALC_CIRC_6\"},\n",
    "            \"reduce_by\":{\"group\", \"name\"},\n",
    "            }\n",
    "        },\n",
    "    recompute=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfoi = df[[\n",
    "    \"timesteps\",\n",
    "    lv.STATES.LONGITUDINAL_DISTANCE_ENDO.value, \n",
    "    lv.STATES.RADIAL_DISTANCE_ENDO_SUPERBASE.value, \n",
    "    lv.STATES.RADIAL_DISTANCE_EPI_SUPERBASE.value,\n",
    "    lv.STATES.WALL_THICKNESS_SUPERBASE.value, \n",
    "    \"global_longitudinal_length_endo_0.0\", \n",
    "    lv.STATES.GLOBAL_CIRCUMFERENTIAL_LENGTH_ENDO_SUPERBASE.value,     \n",
    "    ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare metrics with nodes extracted manually from XPLT file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excelfile = \"C:/Users/igorp/Downloads/manual_results_0.28_70.00_40.00_LVMYO_HEX8_83648_wr.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xls = pd.ExcelFile(excelfile)\n",
    "\n",
    "# prep for storing groups\n",
    "sb_endo_group = []\n",
    "sb_epi_group = []\n",
    "l1_group = []\n",
    "\n",
    "base_group = []\n",
    "apex_group = []\n",
    "\n",
    "# read groups\n",
    "for k in [\"X\", \"Y\", \"Z\"]:\n",
    "    sb_endo_group.append(pd.read_excel(xls, 'SB-ENDO-{}'.format(k)))\n",
    "    sb_epi_group.append(pd.read_excel(xls, 'SB-EPI-{}'.format(k)))\n",
    "    l1_group.append(pd.read_excel(xls, 'L1-{}'.format(k)))\n",
    "    \n",
    "    base_group.append(pd.read_excel(xls, 'BASE-{}'.format(k)))\n",
    "    apex_group.append(pd.read_excel(xls, 'APEX-{}'.format(k)))\n",
    "\n",
    "from_code = dfoi #pd.read_excel(xls, 'from_code')\n",
    "\n",
    "xls.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep data from loaded file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform dfs data into xyz arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_xyz_arr(group_data):\n",
    "    arr = np.asarray([df.values[:, 1:] for df in group_data])\n",
    "    return arr.swapaxes(0, 1).swapaxes(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb_endo = transform_to_xyz_arr(sb_endo_group)\n",
    "sb_epi = transform_to_xyz_arr(sb_epi_group)\n",
    "l1 = transform_to_xyz_arr(l1_group)\n",
    "\n",
    "base = transform_to_xyz_arr(base_group)\n",
    "apex = transform_to_xyz_arr(apex_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid(points, filter=True, ql=0.01, qh=0.99):\n",
    "\n",
    "    if filter:\n",
    "        # remove outliers in x, y and z directions\n",
    "        x = points[:, 0]\n",
    "        y = points[:, 1]\n",
    "        z = points[:, 2]\n",
    "        low_x = np.quantile(x, ql)\n",
    "        high_x = np.quantile(x, qh)\n",
    "        low_y = np.quantile(y, ql)\n",
    "        high_y = np.quantile(y, qh)\n",
    "        low_z = np.quantile(z, ql)\n",
    "        high_z = np.quantile(z, qh)\n",
    "        filter = np.where((x >= low_x) & (x <= high_x) &\n",
    "                        (y >= low_y) & (y <= high_y) &\n",
    "                        (z >= low_z) & (z <= high_z)\n",
    "                        )[0]\n",
    "        bound_points = points[filter]\n",
    "        if len(bound_points) == 0:\n",
    "            bound_points = points\n",
    "    else:\n",
    "        bound_points = points\n",
    "    # compute centroid based on mean of extremas\n",
    "    x = bound_points[:, 0]\n",
    "    y = bound_points[:, 1]\n",
    "    z = bound_points[:, 2]\n",
    "\n",
    "    c = np.zeros(3)\n",
    "    c[0] = (np.max(x) + np.min(x)) * 0.5\n",
    "    c[1] = (np.max(y) + np.min(y)) * 0.5\n",
    "    c[2] = (np.max(z) + np.min(z)) * 0.5\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sb_endo_center = np.asarray([centroid(x) for x in sb_endo])\n",
    "# sb_epi_center = np.asarray([centroid(x) for x in sb_epi])\n",
    "base_center = np.asarray([centroid(x) for x in base])\n",
    "apex_center = np.asarray([centroid(x) for x in apex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = lv.timesteps()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = pd.DataFrame()\n",
    "df_comp[\"timesteps\"] = timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_error(actual, expected):\n",
    "    return ((actual - expected)/expected) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longitudinal distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute longitudinal distance (Base->Apex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ld = np.linalg.norm(base_center - apex_center, axis=1)\n",
    "df_comp[\"ld_manual\"] = ld\n",
    "df_comp[\"ld_code\"] = from_code[\"longitudinal_distance_endo\"]\n",
    "df_comp[\"ld_rel_error\"] = rel_error(from_code[\"longitudinal_distance_endo\"], ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_comp.plot(x=\"timesteps\", y=[\"ld_manual\", \"ld_code\"], style=[\"-\", \"--\"], figsize=(10,5))\n",
    "ax2 = ax.twinx()\n",
    "df_comp.plot(\n",
    "    x=\"timesteps\", \n",
    "    y=[\"ld_rel_error\"], \n",
    "    style=[\"-.\"],\n",
    "    color=[\"purple\"],\n",
    "    ax=ax2, )\n",
    "\n",
    "ax2.set_ylim([-2, 2])\n",
    "ax.set_ylabel(\"longitudinal_distance_endo\")\n",
    "ax2.set_ylabel(\"relative error [%]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute radial distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_from_line(p1:np.ndarray, p2:np.ndarray, p3:np.ndarray, dtype:np.dtype=np.float64) -> np.ndarray:\n",
    "    \"\"\"Computes the perpendicular distance between one or multiple points [p1] to a line (or lines) \\\n",
    "        defined by [p2, p3]. p2 and p3 must have same shape. points are defined as: [x,y,z].\n",
    "\n",
    "    Args:\n",
    "        p1 (np.ndarray): Reference point(s)\n",
    "        p2 (np.ndarray): First point on line(s)\n",
    "        p3 (np.ndarray): Second point on line(s)\n",
    "        dtype (np.dtype, optional): Values are converted to numpy array. This denies the output type. Defaults to np.float64.\n",
    "\n",
    "    Returns:\n",
    "        float or np.ndarray: Perpendicular distance(s). If singular point and line, will return float. Otherwise will return an array.\n",
    "    \"\"\"\n",
    "    assert len(p2) == len(p3), \"p2 and p3 must have same number of points as they represent lines. Received: p2 ({}), p3 ({})\".format(len(p2), len(p3))\n",
    "    if not isinstance(p1, np.ndarray):\n",
    "        p1 = np.asarray(p1, dtype=dtype)\n",
    "        assert p1.shape[-1] == 3, \"Point must be composed of [x,y,z]. Received shape (p1): {}\".format(p1.shape)\n",
    "    if not isinstance(p2, np.ndarray):\n",
    "        p2 = np.asarray(p2, dtype=dtype)\n",
    "        assert p2.shape[-1] == 3, \"Point must be composed of [x,y,z]. Received shape (p2): {}\".format(p2.shape)\n",
    "    if not isinstance(p3, np.ndarray):\n",
    "        p3 = np.asarray(p3, dtype=dtype)\n",
    "        assert p3.shape[-1] == 3, \"Point must be composed of [x,y,z]. Received shape (p3): {}\".format(p3.shape)\n",
    "\n",
    "    dists = np.linalg.norm(np.cross(p3-p2, p2-p1, axis=-1), axis=-1)/np.linalg.norm(p3-p2, axis=-1)\n",
    "    # dists = np.linalg.norm(np.cross(p2-p1, p1-p3, axis=-1), axis=-1)/np.linalg.norm(p2-p1, axis=-1)\n",
    "    return dists.astype(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rd_endo = []\n",
    "rd_epi = []\n",
    "for i in range(45):\n",
    "    # rs_endo = np.linalg.norm(sb_endo[i, :] - sb_endo_la_center[i], axis=1)\n",
    "    rd_endo.append(dist_from_line(sb_endo[i], base_center[i], apex_center[i]))\n",
    "    rd_epi.append(dist_from_line(sb_epi[i], base_center[i], apex_center[i]))\n",
    "    \n",
    "rd_endo = np.mean(rd_endo, axis=1)\n",
    "rd_epi = np.mean(rd_epi, axis=1)\n",
    "\n",
    "df_comp[\"rd_endo_manual\"] = rd_endo\n",
    "df_comp[\"rd_endo_code\"] = from_code[\"radial_distance_endo_superbase\"]\n",
    "df_comp[\"rd_endo_error\"] = rel_error(from_code[\"radial_distance_endo_superbase\"], rd_endo)\n",
    "\n",
    "df_comp[\"rd_epi_manual\"] = rd_epi\n",
    "df_comp[\"rd_epi_code\"] = from_code[\"radial_distance_epi_superbase\"]\n",
    "df_comp[\"rd_epi_error\"] = rel_error(from_code[\"radial_distance_epi_superbase\"], rd_epi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_comp.plot(\n",
    "    x=\"timesteps\", \n",
    "    y=[\"rd_endo_manual\", \"rd_endo_code\", \"rd_epi_manual\", \"rd_epi_code\"], \n",
    "    style=[\"-\", \"--\", \"-\", \"--\"],\n",
    "    figsize=(10,5))\n",
    "\n",
    "ax2 = ax.twinx()\n",
    "df_comp.plot(\n",
    "    x=\"timesteps\", \n",
    "    y=[\"rd_endo_error\", \"rd_epi_error\"], \n",
    "    style=[\"-.\", \"-.\"],\n",
    "    color=[\"purple\", \"cyan\"],\n",
    "    ax=ax2, )\n",
    "\n",
    "ax2.set_ylim([-2, 2])\n",
    "ax.set_ylabel(\"radial_distance_superbase\")\n",
    "ax2.set_ylabel(\"relative error [%]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wall thickness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute wall thickness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = rd_epi - rd_endo\n",
    "df_comp[\"wt_manual\"] = wt\n",
    "df_comp[\"wt_code\"] = from_code[\"wall_thickness_superbase\"]\n",
    "df_comp[\"wt_rel_error\"] = rel_error(from_code[\"wall_thickness_superbase\"], wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_comp.plot(\n",
    "    x=\"timesteps\", \n",
    "    y=[\"wt_manual\", \"wt_code\"], \n",
    "    style=[\"-\", \"--\"],\n",
    "    figsize=(10,5))\n",
    "ax2 = ax.twinx()\n",
    "df_comp.plot(\n",
    "    x=\"timesteps\", \n",
    "    y=[\"wt_rel_error\"], \n",
    "    style=[\"-.\"],\n",
    "    color=[\"purple\"],\n",
    "    ax=ax2, )\n",
    "\n",
    "ax2.set_ylim([-2, 2])\n",
    "ax.set_ylabel(\"wall_thickness_superbase\")\n",
    "ax2.set_ylabel(\"relative error [%]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Circumferential length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute circumferential length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 40\n",
    "clustering = KMeans(n_clusters=n_clusters, random_state=0).fit(sb_endo[0, :, :2])\n",
    "labels = clustering.labels_\n",
    "\n",
    "centers = []\n",
    "for k in range(n_clusters):\n",
    "    mask = np.where(labels==k)[0]\n",
    "    centers.append([np.mean(x[mask], axis=1) for x in sb_endo])\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(sb_endo[0, :, 0], sb_endo[0, :, 1], c=labels)\n",
    "# plt.scatter(sb_endo[0, :, 0], sb_endo[0, :, 1], c=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_spherical(xyz):\n",
    "    # transform to spherical coordinates\n",
    "    rs = np.linalg.norm(xyz, axis=1)\n",
    "    thetas = np.arccos(xyz[:, 2]/rs)\n",
    "    phis = np.arctan2(xyz[:, 1],xyz[:, 0])\n",
    "    # sort by columns\n",
    "    ids = np.lexsort((rs, thetas, phis))\n",
    "    return xyz[ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = []\n",
    "for x in sb_endo:\n",
    "    c = np.asarray([np.mean(x[np.where(labels==k)[0]], axis=0) for k in range(n_clusters)])\n",
    "    centers.append(sort_by_spherical(c))\n",
    "centers = np.asarray(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(sb_endo[0, :, 0], sb_endo[0, :, 1])\n",
    "plt.scatter(centers[0, :, 0], centers[0, :, 1], c=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(centers[0, :, 0], centers[0, :, 1], c=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_sum(coords, join_ends=False):\n",
    "    if not join_ends:\n",
    "        return np.sum(np.linalg.norm(coords[1:]-coords[:-1], axis=1))\n",
    "    else:\n",
    "        _coords = np.vstack((coords, coords[0]))\n",
    "        return np.sum(np.linalg.norm(_coords[1:]-_coords[:-1], axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = np.asarray([line_sum(x, join_ends=True) for x in centers])\n",
    "df_comp[\"sc_manual\"] = sc\n",
    "df_comp[\"sc_code\"] = from_code[\"global_circumferential_length_endo_superbase\"]\n",
    "df_comp[\"sc_rel_error\"] = rel_error(from_code[\"global_circumferential_length_endo_superbase\"], sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_comp.plot(\n",
    "    x=\"timesteps\", \n",
    "    y=[\"sc_manual\", \"sc_code\"], \n",
    "    style=[\"-\", \"--\"],\n",
    "    figsize=(10,5))\n",
    "ax2 = ax.twinx()\n",
    "df_comp.plot(\n",
    "    x=\"timesteps\", \n",
    "    y=[\"sc_rel_error\"], \n",
    "    style=[\"-.\"],\n",
    "    color=[\"purple\"],\n",
    "    ax=ax2, )\n",
    "\n",
    "ax2.set_ylim([-2, 2])\n",
    "ax.set_ylabel(\"global_circumferential_length_endo_superbase\")\n",
    "ax2.set_ylabel(\"relative error [%]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Longitudinal length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Longitudinal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 30\n",
    "clustering = KMeans(n_clusters=n_clusters, random_state=0).fit(l1[0, :])\n",
    "labels = clustering.labels_\n",
    "\n",
    "centers = []\n",
    "for k in range(n_clusters):\n",
    "    mask = np.where(labels==k)[0]\n",
    "    centers.append([np.mean(x[mask], axis=1) for x in l1])\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "plt.scatter(l1[0, :, 1], l1[0, :, 2], c=labels)\n",
    "# plt.scatter(sb_endo[0, :, 0], sb_endo[0, :, 1], c=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from project_heart.utils.vector_utils import angle_between, project_pts_onto_plane, calc_plane_d\n",
    "\n",
    "normal = [1,0,0] # plane normal\n",
    "ref_vector = [0,0,1] # vec to compute angles (so we can sort)\n",
    "ref_center = np.mean(l1[0, :], axis=0) # center of pts\n",
    "\n",
    "# project points into plane\n",
    "plane_d = calc_plane_d(normal, ref_center)\n",
    "ppts = project_pts_onto_plane(l1[0, :], normal, plane_d)\n",
    "p_center = project_pts_onto_plane(ref_center, normal, plane_d)[0]\n",
    "\n",
    "# get vectors from center to projected points\n",
    "vecs = clustering.cluster_centers_ - p_center \n",
    "\n",
    "# compute angles\n",
    "angles = angle_between(vecs, ref_vector, check_orientation=True, zaxis=normal) \n",
    "# sort angles\n",
    "idx = np.argsort(angles)\n",
    "# reconstruct labels\n",
    "labels_order = np.zeros_like(idx)\n",
    "labels_order[idx] = np.arange(n_clusters)\n",
    "new_labels = np.asarray([labels_order[v] for v in labels])\n",
    "labels = new_labels\n",
    "\n",
    "# plot \n",
    "plt.scatter(l1[6, :, 1], l1[6, :, 2], c=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = []\n",
    "for x in l1:\n",
    "    c = np.asarray([np.mean(x[np.where(labels==k)[0]], axis=0) for k in range(n_clusters)])\n",
    "    # centers.append(sort_by_zs_ys(c))\n",
    "    centers.append(c)\n",
    "centers = np.asarray(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(l1[6, :, 1], l1[6, :, 2])\n",
    "plt.scatter(centers[6, :, 1], centers[6, :, 2], c=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(centers[6, :, 1], centers[6, :, 2], c=\"red\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = np.asarray([line_sum(x) for x in centers])\n",
    "\n",
    "df_comp[\"sl_manual\"] = sl\n",
    "df_comp[\"sl_code\"] = from_code[\"global_longitudinal_length_endo_0.0\"]\n",
    "df_comp[\"sl_rel_error\"] = rel_error(from_code[\"global_longitudinal_length_endo_0.0\"], sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_comp.plot(\n",
    "    x=\"timesteps\", \n",
    "    y=[\"sl_manual\", \"sl_code\"], \n",
    "    style=[\"-\", \"--\"],\n",
    "    figsize=(10,5))\n",
    "ax2 = ax.twinx()\n",
    "df_comp.plot(\n",
    "    x=\"timesteps\", \n",
    "    y=[\"sl_rel_error\"], \n",
    "    style=[\"-.\"],\n",
    "    color=[\"purple\"],\n",
    "    ax=ax2)\n",
    "\n",
    "ax2.set_ylim([-2, 2])\n",
    "ax.set_ylabel(\"global_longitudinal_length\")\n",
    "ax2.set_ylabel(\"relative error [%]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if we are comparing same nodes (from manual and speckles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spks_code = lv.nodes(mask=spks4.stack_ids())\n",
    "spks_manual = l1[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter = pv.Plotter()\n",
    "plotter.add_mesh(pv.PolyData(spks_code), render_points_as_spheres=True, color=\"red\", opacity=0.5)\n",
    "plotter.add_mesh(pv.PolyData(spks_manual), render_points_as_spheres=True, color=\"yellow\", opacity=0.5)\n",
    "plotter.show_grid()\n",
    "plotter.show(jupyter_backend='static')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('project_heart')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a4b270fe5129ef310552b5197d637a230d47119f1e46f0b92ff32b1c5f5c5425"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
